{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12698/752784679.py:12: DeprecationWarning: Please use `gaussian_filter` from the `scipy.ndimage` namespace, the `scipy.ndimage.filters` namespace is deprecated.\n",
      "  from scipy.ndimage.filters import gaussian_filter\n",
      "2024-12-05 10:20:03.311496: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-05 10:20:03.336513: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-05 10:20:03.796320: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.16.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import math\n",
    "from IPython.display import Markdown, display\n",
    "import importlib\n",
    "import json\n",
    "from tabulate import tabulate\n",
    "import colorsys\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "import sklearn\n",
    "import matplotlib\n",
    "import xgboost as xgb\n",
    "import random\n",
    "from tqdm import tqdm as tqdm\n",
    "\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import src.cropnet as cropnet\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "    \n",
    "import tensorflow as tf\n",
    "tf.version.VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/full_data/processed/processed_merged.feather\"\n",
    "df = pd.read_feather(path)\n",
    "f = open(path.replace(\".feather\", \".json\"), \"r\")\n",
    "correlation_filters = json.load(f)\n",
    "f.close()\n",
    "if 'level_0' in df.columns:\n",
    "    df = df.drop(columns=['level_0'])\n",
    "df = df.reset_index()\n",
    "if 'level_0' in df.columns:\n",
    "    df = df.drop(columns=['level_0'])\n",
    "\n",
    "df = df[np.isnan(df['yield']) == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'yield_1'\n",
    "predictors = [c for c in correlation_filters['base'] if c not in ['index', 'x', 'y', 'x_int', 'y_int', 'polygon_id', 'grdkod_mar'] and \"yield\" not in c and \"_pid\" not in c]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mX = np.mean(df[predictors], axis=0)\n",
    "sX = np.std(df[predictors], axis=0)\n",
    "sX[sX == 0] = 1\n",
    "\n",
    "mY = np.mean(df[target], axis=0)\n",
    "sY = np.std(df[target], axis=0)\n",
    "\n",
    "norm_info = {\n",
    "    'X': {'mean': mX, 'std': sX},\n",
    "    'y': {'mean': mY, 'std': sY}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 152/152 [00:06<00:00, 24.13it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = []\n",
    "for pid in tqdm(set( df['matching_pid'])):\n",
    "    pdf = df[df['matching_pid'] == pid]\n",
    "    X = ((pdf.loc[df['matching_pid'] == pid, predictors]-mX)/sX).values\n",
    "    y = ((pdf.loc[df['matching_pid'] == pid, target]-mY)/sY).values.reshape((-1, 1))\n",
    "    info = {\n",
    "        'x': pdf.x.values,\n",
    "        'y': pdf.y.values,\n",
    "        'pid': pid\n",
    "    }\n",
    "    dataset.append((X, y, info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "x_int = ((info['x'] - np.min(info['x']))/10).astype(int)\n",
    "np.max(x_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_CNN = []\n",
    "for X, y, info in dataset:\n",
    "    d = 3\n",
    "    \n",
    "    x_int = ((info['x'] - np.min(info['x']))/10).astype(int)\n",
    "    y_int = ((info['y'] - np.min(info['y']))/10).astype(int)\n",
    "    w = np.max(x_int)+1+2*d\n",
    "    h = np.max(y_int)+1+2*d\n",
    "\n",
    "    imgX = np.zeros((w, h, X.shape[1]))\n",
    "    imgX[x_int+d, y_int+d, :] = X\n",
    "    imgY = np.zeros((w, h))\n",
    "    imgY[x_int+d, y_int+d] = y.reshape(-1)\n",
    "\n",
    "    IMG_BATCH = np.zeros((y.shape[0]//50, X.shape[1], 2*d+1, 2*d+1))\n",
    "    IMG_Y = np.zeros((y.shape[0]//50, 1))\n",
    "    xs = np.zeros(y.shape[0]//50)\n",
    "    ys = np.zeros(y.shape[0]//50)\n",
    "    for i in range(IMG_BATCH.shape[0]):\n",
    "        idx = random.randint(0, X.shape[0]-1)\n",
    "        x = x_int[idx]+d\n",
    "        y = y_int[idx]+d\n",
    "        IMG_BATCH[i, :, :, :] = np.transpose(imgX[x-d:x+d+1, y-d:y+d+1, :], (2,0,1))\n",
    "        IMG_Y[i] = imgY[x, y]\n",
    "        xs[i] = info['x'][idx]\n",
    "        ys[i] = info['y'][idx]\n",
    "        \n",
    "    IMG_BATCH = torch.tensor(IMG_BATCH, device=device, dtype=torch.float)\n",
    "    IMG_Y = torch.tensor(IMG_Y, device=device, dtype=torch.float)\n",
    "    info = {\n",
    "        'x': xs,\n",
    "        'y': ys,\n",
    "        'pid': pid\n",
    "    }\n",
    "    dataset_CNN.append((IMG_BATCH, IMG_Y, info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(dataset, p_val, p_test, seed=-1):\n",
    "    \n",
    "    N = np.sum([X.shape[0] for X, y, info in dataset])\n",
    "    rem = [data for data in dataset]\n",
    "    \n",
    "    data_val = []\n",
    "    val_size = 0\n",
    "    while val_size/N < p_val:\n",
    "        idx = random.choice(range(len(rem)))\n",
    "        data = rem.pop(idx)\n",
    "        data_val.append(data)\n",
    "        val_size += data[0].shape[0]\n",
    "        \n",
    "        \n",
    "    data_test = []\n",
    "    test_size = 0\n",
    "    while test_size/N < p_test:\n",
    "        idx = random.choice(range(len(rem)))\n",
    "        data = rem.pop(idx)\n",
    "        data_test.append(data)\n",
    "        test_size += data[0].shape[0]\n",
    "        \n",
    "    return rem, data_val, data_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = dataset[0][0].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "223"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M//4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─MaxPool2d: 1-1                         --\n",
      "├─ModuleList: 1-2                        --\n",
      "|    └─Conv2d: 2-1                       3,580,934\n",
      "|    └─Conv2d: 2-2                       895,345\n",
      "├─Linear: 1-3                            14,336\n",
      "├─Linear: 1-4                            65\n",
      "=================================================================\n",
      "Total params: 4,490,680\n",
      "Trainable params: 4,490,680\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "class Conv(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Conv, self).__init__()\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.conv_layers = torch.nn.ModuleList([\n",
    "            nn.Conv2d(M, M//2, 3, padding=1),\n",
    "            nn.Conv2d(M//2, M//4, 3, padding=1)\n",
    "        ])\n",
    "        self.dense_layer = nn.Linear(M//4, 64)\n",
    "        self.output = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = data\n",
    "        for layer in self.conv_layers:\n",
    "            x = layer(x)\n",
    "            x = self.pool(nn.functional.relu(x))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.dense_layer(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "model = Conv()\n",
    "summary(model);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def run_model(model, data_train, data_val, data_test, epochs=100, lr=0.001):\n",
    "    \n",
    "\n",
    "    history = {\n",
    "        'train': [],\n",
    "        'val': [],\n",
    "        'test': []\n",
    "    }\n",
    "\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "\n",
    "    scores = {}\n",
    "    top_score = 10000\n",
    "\n",
    "    # Training loop\n",
    "    model.train()\n",
    "\n",
    "    pbar = tqdm(range(epochs))\n",
    "    for epoch in pbar:\n",
    "        \n",
    "        random.shuffle(data_train)\n",
    "        err = np.zeros(0)\n",
    "        total_los = []\n",
    "        for X, y, info in data_train:\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(X)\n",
    "            y_cpu = y.clone().cpu().detach().numpy()\n",
    "            pred_cpu = pred.clone().cpu().detach().numpy()\n",
    "            y_gpu = torch.tensor(y_cpu).to(device)\n",
    "            loss = criterion(pred, y_gpu)\n",
    "            loss.backward()\n",
    "            total_los.append(loss.item())\n",
    "            optimizer.step()\n",
    "            err = np.concatenate([err, cropnet.eval_error(pred, y, info, norm_info)])\n",
    "        train_rmse = np.sqrt(np.nanmean(err**2))\n",
    "        history['train'].append(train_rmse)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        val_rmse, val_err = cropnet.eval_dataset(data_val, model, norm_info)\n",
    "        #val_err = val_err - np.nanmean(val_err)\n",
    "        history['val'].append(val_rmse)\n",
    "        val_acc10 = np.mean((np.abs(val_err) < 1)[np.isnan(val_err) == False])\n",
    "        metric = val_rmse\n",
    "            \n",
    "        test_rmse, test_err = cropnet.eval_dataset(data_test, model, norm_info)\n",
    "        #test_err = test_err - np.nanmean(test_err)\n",
    "        #test_rmse = np.sqrt(np.nanmean(test_err**2))\n",
    "        history['test'].append(test_rmse)\n",
    "        acc5 = np.mean((np.abs(test_err) < 0.5)[np.isnan(test_err) == False])\n",
    "        acc10 = np.mean((np.abs(test_err) < 1)[np.isnan(test_err) == False])\n",
    "        acc20 = np.mean((np.abs(test_err) < 2)[np.isnan(test_err) == False])\n",
    "\n",
    "        if metric < top_score:\n",
    "            top_score = metric\n",
    "            torch.save(model, \"/\".join([\"models\", \"current.pth\"]))\n",
    "            scores = {\n",
    "                'test_rmse': test_rmse,\n",
    "                'test_acc_0.5': acc5,\n",
    "                'test_acc_1.0': acc10, \n",
    "                'test_acc_2.0': acc20,\n",
    "                'test_rel_rmse': test_rmse/norm_info['y']['mean'],\n",
    "            }\n",
    "\n",
    "        pbar.set_description(f'Loss: {np.round(np.mean(total_los), 3)}, train rmse: {np.round(train_rmse, 3)}, val rmse: {np.round(val_rmse, 3)}, test rmse: {np.round(test_rmse, 3)}, top: [{np.round(metric, 3)} >= {np.round(top_score, 3)} {[np.round(scores[s], 3) for s in scores]}]')\n",
    "\n",
    "    model = torch.load(\"models/current.pth\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.089, train rmse: 0.845, val rmse: 1.023, test rmse: 1.189, top: [1.023 >= 0.907 [1.025, 0.484, 0.699, 0.939, 0.124]]: 100%|██████████| 100/100 [00:40<00:00,  2.50it/s]\n",
      "Loss: 0.061, train rmse: 0.68, val rmse: 0.961, test rmse: 1.169, top: [0.961 >= 0.841 [1.313, 0.366, 0.619, 0.898, 0.159]]: 100%|██████████| 100/100 [00:39<00:00,  2.55it/s]\n",
      "Loss: 0.058, train rmse: 0.638, val rmse: 1.102, test rmse: 1.221, top: [1.102 >= 0.946 [1.497, 0.343, 0.617, 0.879, 0.181]]: 100%|██████████| 100/100 [00:38<00:00,  2.57it/s]\n",
      "Loss: 0.319, train rmse: 1.509, val rmse: 1.256, test rmse: 1.115, top: [1.256 >= 0.965 [1.05, 0.388, 0.701, 0.935, 0.127]]: 100%|██████████| 100/100 [00:38<00:00,  2.60it/s]\n",
      "Loss: 0.166, train rmse: 1.047, val rmse: 1.066, test rmse: 1.284, top: [1.066 >= 0.991 [1.193, 0.366, 0.645, 0.894, 0.145]]: 100%|██████████| 100/100 [00:39<00:00,  2.54it/s]\n",
      "Loss: 0.32, train rmse: 1.373, val rmse: 1.234, test rmse: 1.598, top: [1.234 >= 1.005 [1.141, 0.41, 0.699, 0.908, 0.138]]: 100%|██████████| 100/100 [00:39<00:00,  2.53it/s] \n",
      "Loss: 0.979, train rmse: 2.179, val rmse: 2.156, test rmse: 2.238, top: [2.156 >= 0.966 [1.208, 0.353, 0.637, 0.888, 0.146]]: 100%|██████████| 100/100 [00:36<00:00,  2.72it/s]\n",
      "Loss: 0.06, train rmse: 0.704, val rmse: 1.18, test rmse: 1.215, top: [1.18 >= 1.18 [1.215, 0.346, 0.65, 0.894, 0.147]]: 100%|██████████| 100/100 [00:39<00:00,  2.54it/s]    \n",
      "Loss: 0.081, train rmse: 0.808, val rmse: 1.058, test rmse: 1.1, top: [1.058 >= 0.997 [1.034, 0.46, 0.742, 0.932, 0.125]]: 100%|██████████| 100/100 [00:40<00:00,  2.48it/s]  \n",
      "Loss: 0.08, train rmse: 0.757, val rmse: 1.241, test rmse: 1.072, top: [1.241 >= 1.087 [1.001, 0.455, 0.735, 0.945, 0.121]]: 100%|██████████| 100/100 [00:40<00:00,  2.49it/s]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "epochs = 100\n",
    "\n",
    "score_list = []\n",
    "for i in range(10):\n",
    "    data_train, data_val, data_test = split_data(dataset_CNN, 0.15, 0.15)\n",
    "    M = dataset_CNN[0][0].shape[1]\n",
    "    model = Conv()\n",
    "    model.to(device)\n",
    "    model = run_model(model, data_train, data_val, data_test, epochs=100, lr=0.001)\n",
    "\n",
    "    \n",
    "    y_tot = np.zeros(0)\n",
    "    y_pred = np.zeros(0)\n",
    "    test_err = np.zeros(0)\n",
    "    test_err_norm = np.zeros(0)\n",
    "    pids = np.zeros(0)\n",
    "    for X, y, info in data_test:\n",
    "        pred = model(X)\n",
    "        \n",
    "        yp = pred.cpu().detach().numpy().reshape(-1)\n",
    "        yt = y.cpu().detach().numpy().reshape(-1)\n",
    "        err = (yt - yp)*norm_info['y']['std']\n",
    "        \n",
    "        y_tot = np.concatenate([y_tot, yt])\n",
    "        y_pred = np.concatenate([y_pred, yp])\n",
    "        test_err = np.concatenate([test_err, err])\n",
    "        test_err_norm = np.concatenate([test_err_norm, err - np.nanmean(err)])\n",
    "        pids = np.concatenate([pids, np.ones(err.shape)*info['pid']])\n",
    "        \n",
    "    y_tot = y_tot*norm_info['y']['std'] + norm_info['y']['mean']\n",
    "    scores_unnorm = [np.sqrt(np.nanmean(test_err**2)), np.nanmean((np.abs(test_err) < 1)[np.isnan(test_err) == False]), np.nanmean(np.abs(test_err/y_tot)), 1-np.sum(test_err**2)/np.sum((y_tot-np.mean(y_tot))**2)]\n",
    "    scores_norm = [np.sqrt(np.nanmean(test_err_norm**2)), np.nanmean((np.abs(test_err_norm) < 1)[np.isnan(test_err_norm) == False]), np.nanmean(np.abs(test_err_norm/y_tot)), 1-np.sum(test_err_norm**2)/np.sum((y_tot-np.mean(y_tot))**2)]\n",
    "    result = [scores_unnorm, scores_norm, [list(y_tot), list(y_pred), list(test_err), list(test_err_norm), list(pids)]]\n",
    "    score_list.append(result)\n",
    "\n",
    "\n",
    "cropnet.save_score_to_json(score_list, \"results_final/other_models/CNN.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
